# Running Qwen3-14B Locally on a Pixel 9 Pro: Options & Tools

## Termux-Based Solutions (On-Device Servers)

These methods use Termux (a Linux-like environment on Android) to run LLM inference and serve it over a local network.

### **Ollama in Termux (Local LLM Framework)**

* **Setup:** Install Termux (preferably F-Droid version) and build Ollama from source in Termux (via Go). Community guides show successful builds on Pixel devices (e.g. Pixel 7 Pro) with Termux. Once built, start the Ollama background server (`ollama serve`).
* **Supported Model Format:** Ollama manages its own model downloads/quantizations (built on GGML/GGUF under the hood). It can directly fetch and run models like Llama 3 and Qwen in quantized form.
* **Qwen3 14B Support:** Yes – Ollama explicitly supports Alibaba’s Qwen models. For example, “Qwen 3” is featured as a runnable model. You can `ollama pull` the Qwen3-14B model (in a compatible quantized format) and run it on-device.
* **API Access:** Ollama includes a RESTful API by default. When the Ollama server is running, it listens (by default on `127.0.0.1:11434`) for API calls, similar to OpenAI’s API format. You can configure Termux to allow external access (e.g. via WiFi IP) so other LAN devices can query the model.
* **Resource Needs & Limits:** CPU-only inference (no GPU acceleration on Pixel). A 14B model in 4-bit quantization (\~7–8 GB) will *just* fit in a Pixel 9 Pro’s RAM (likely \~12 GB) – expect slow generation (on the order of 0.5–2 tokens/sec). Smaller models (2–7B) are recommended for reasonable speed. Ensure Termux has a wake-lock to prevent Android from killing the session during long runs.
* **Stability & Support:** Moderate. Ollama is a relatively new open-source project but growing fast. Linux/Android support is community-driven (not officially listed on the Ollama site, which highlights desktop OS) – however, multiple users have documented the process, and updates continue to improve it. Expect to do some tweaking when building, but once running it is stable for on-device chats. Community forums (Reddit, GitHub issues) have active discussions on Termux use.

### **Llama.cpp HTTP Server (GGUF Models via C++ or Python)**

* **Setup:** Compile the latest **llama.cpp** in Termux (with NEON optimizations for ARM). This yields a lightweight CLI that can run models and even serve them. The official llama.cpp project now offers an **OpenAI-compatible HTTP server** mode. You can either use the C++ server (if enabled) or the Python bindings. For instance, installing `llama-cpp-python[server]` and running `python3 -m llama_cpp.server --model /path/to/model.gguf` will launch a local API server.
* **Supported Model Format:** **GGUF** (the universal GGML format). Models must be converted to GGUF (tools exist to convert HuggingFace weights). Many popular LLMs (Llama 2, Mistral, Qwen, etc.) have pre-quantized GGUF files available.
* **Qwen3 14B Support:** **Yes (with conversion).** The llama.cpp engine can handle architectures beyond Llama, including Qwen, if the model is in GGUF. The Qwen-14B model has been successfully converted to GGUF by community projects (e.g. Unsloth’s Qwen3-14B GGUF on HuggingFace). Once converted, you can load it with llama.cpp. Keep in mind Qwen’s 8K context might further tax memory.
* **API Access:** **Yes.** The llama.cpp server (or its Python API wrapper) provides a REST endpoint implementing OpenAI’s `/v1/completions` and chat API. This means other devices/apps can query the Pixel as if it were an OpenAI server. You’ll typically bind to `0.0.0.0:port` in Termux for LAN access. This is a fully offline, self-hosted solution.
* **Resource Needs & Limits:** CPU-bound inference using all big cores via threads. A 14B model at 4-bit will consume \~7–8 GB RAM plus overhead; 8-bit quant will double that. The Pixel’s Tensor chipset lacks heavy-duty CPU oomph compared to a PC, so generation is slow. Running multi-user or concurrent requests is possible (llama.cpp’s server supports a few parallel sessions), but each will be slow. For better performance, consider smaller models or quantization to 3-bit (if available) at quality cost.
* **Stability & Support:** **High.** Llama.cpp is a very mature and active project with a large community. It’s known to compile and run on aarch64 Linux without issues (users even ran it on Raspberry Pi and Android via Termux). The HTTP server feature is relatively new but built on robust foundations. Community support (Discord, GitHub) is excellent, and many guides exist for llama.cpp usage. This is a reliable choice for turning your Pixel into a mini AI server.

### **LocalAI (OpenAI-API Emulator in Go)**

* **Setup:** **LocalAI** is a drop-in replacement for the OpenAI API, running fully locally. It’s written in Go, so you can compile it on Termux similarly to Ollama (or cross-compile a binary). With Go installed in Termux, clone the `mudler/LocalAI` repo and `go build` it – producing a single binary. Run this server binary in Termux to start the API (no Docker needed on-device, though many PC guides use Docker).
* **Supported Model Formats:** Primarily **GGML/GGUF** quantized models (the same as llama.cpp). LocalAI uses the GGML backend for LLMs and can also interface with GPT4All models, LLaMA, Mistral, etc. (It also supports some fine-tunes and LoRAs via configuration.) In the latest releases, **Qwen** and other new model architectures are supported.
* **Qwen3 14B Support:** **Yes.** LocalAI added support for Qwen3 models as of version 2.29.0. You can place a Qwen-14B GGUF model in its `models` folder and configure it. This enables running Qwen3-14B and even newer architectures behind a uniform API.
* **API Access:** **Yes.** LocalAI runs an **OpenAI-compatible HTTP REST API** (default at `http://localhost:8080/v1` endpoints). Any LAN device can POST to this API (e.g. using the OpenAI Python client with the base URL pointed to your phone). It supports completions, chat, embeddings, etc., making your Pixel a true local AI service.
* **Resource Needs & Limits:** Similar to llama.cpp since it uses the same backend. It will use CPU threads for inference. Ensure you have enough storage for model files and sufficient free RAM (a swap file in Termux may help with very large models, at performance cost). The Pixel’s battery will drain quickly under full-load; running while plugged in is advised for longer sessions. LocalAI can be configured to run multiple models, but on a phone it’s best to stick to one loaded model at a time due to memory constraints.
* **Stability & Support:** **Moderate.** LocalAI is an active open-source project with frequent updates and a modest community. It’s proven on Linux x86; on Android aarch64 it’s less common, but being Go-based it should run reliably once compiled. The advantage is convenience (auto-handling API requests, formatting, etc.), but the trade-off is a slightly larger footprint than raw llama.cpp. Documentation is decent, and issues/bug reports are handled on GitHub. For a largely headless, API-centric setup, LocalAI is a strong option.

*Aside:* *You could also run other server projects (like the **Text-Generation-WebUI** or **FastChat** in a Termux proot container), but those are heavyweight (requiring Python + dependencies) and not as optimized for mobile. The above solutions are purpose-built for lightweight local inference.*

## Native Android Apps (On-Device Only, Mostly No Network API)

These apps let you run or chat with local models on Android. They are easy to set up and offline, but generally **do not natively expose an HTTP API** (they’re meant for direct use on the device). You might use these if you only need on-phone AI or are willing to build a custom bridge for network access.

### **PocketPal AI** (Android/iOS app, open-source)

* **Setup:** Install from Google Play or build from the [PocketPal GitHub](https://github.com/a-ghorbani/pocketpal-ai). Once installed, you can download models within the app’s interface. PocketPal integrates with Hugging Face Hub to let you **browse and download models** directly on the phone.
* **Supported Model Formats:** PocketPal uses GGML/GGUF quantized models for efficiency. The app can fetch models like **Danube**, **Phi**, **Gemma**, and **Qwen** in small sizes. It supports swapping multiple models and will offload from RAM when not in use to manage memory.
* **Qwen3 14B Support:** **Partial.** The app’s model list mentions *“Qwen”*, which indicates support for some Qwen family models. In practice, PocketPal is optimized for **Small Language Models (SLMs)** – e.g. 1B–7B parameters. A full 14B Qwen3 might be too large to run comfortably. However, if you obtain a Qwen3-14B GGUF and the device has enough RAM, the app *could* attempt to load it. Users have reported success mainly with smaller models (e.g. Qwen-7B or Qwen-1.8B).
* **API Access:** **No (not by default).** PocketPal is a mobile chat interface – it does not provide an HTTP server for external devices. It’s designed for on-device private use. (One could theoretically script UI interactions or modify the open-source code to expose a port, but that’s non-trivial.)
* **Resource Requirements:** **High for large models.** PocketPal was designed with efficiency in mind, but phones still struggle beyond a few billion parameters. On a modern Snapdragon 8 Gen chip, users report \~10–12 tokens/sec with a 3B model. A 14B model would be much slower (<3 tokens/sec) and may risk out-of-memory on 12 GB RAM. The app’s auto-offloading helps prevent crashes by unloading models when not active. Expect to stick to smaller models for fluid conversations.
* **Stability & Community:** **High.** The app is actively maintained (frequent updates, e.g. v1.9 in Apr 2025) and has a growing user community. Being open-source, it has transparency and community contributions. Many consider it one of the most polished mobile LLM apps, with features like chat history, editable messages, and even benchmarking built-in. For personal offline AI use, it’s a top choice, but again – it’s not an out-of-the-box network server.

### **ChatterUI** (Android app, open-source)

* **Setup:** Download the APK from the [ChatterUI releases](https://github.com/Vali-98/ChatterUI/releases) and install. The app is a “bring your own model” frontend. After enabling *Local Mode*, you import a model file into the app (or point it to an external model path). Once a GGUF model is loaded, you can chat with it entirely offline.
* **Supported Model Formats:** Uses **llama.cpp under the hood**. ChatterUI is essentially a React Native frontend paired with a native llama.cpp backend. It supports **GGUF** models (any that fit in RAM). This includes LLaMA-family models and others that have GGUF conversions.
* **Qwen3 14B Support:** **Yes (if you have the file).** There is no built-in model download list here; you supply the model. If you convert Qwen3-14B to GGUF and have enough memory, ChatterUI will attempt to run it. The app itself doesn’t limit the model choices – users have run 13B+ models on powerful devices. Keep in mind 14B is borderline on 12 GB RAM; you might need to use a 4-bit quantization and accept slow responses.
* **API Access:** **No.** ChatterUI is a client interface, not a server. It actually can act as a **client** to remote APIs as well – in “Remote Mode” you can connect it to an existing API (OpenAI, Ooba text-gen, etc.) instead of local model. But it does not itself expose a network API for others. It’s meant for interacting directly on the phone or as a UI to another server.
* **Resource Requirements:** Similar to PocketPal. ChatterUI gives fine-grained controls (e.g. adjusting context length, sampling parameters) which advanced users can tweak for performance. It doesn’t automatically offload models, so you must be cautious to load models that fit in RAM. The UI is fairly minimal, but running a big model will heat up and slow down your Pixel. For practical use, 7B or smaller models make sense; 13–14B models will be very slow and may require killing background apps to free memory.
* **Stability & Community:** **Good.** The developer is responsive (app is AGPL-licensed) and the community on Reddit has provided feedback and fixes. It’s not as polished as PocketPal (e.g. some users noted performance issues in certain versions), but it’s improving. A bonus is **character card** support and multi-chat management, appealing to roleplay or persona-based use cases. Overall, ChatterUI is a flexible alternative, especially if you want one app that can either run models locally or connect to your home server.

### **Llamao** (Android/iOS app, closed-source)

* **Setup:** Install from the Play Store or App Store (it’s a commercial app with a free tier). By default, Llamao comes with one pre-loaded “powerful model” free, and offers premium upgrades for additional models or features. No account or internet is needed once it’s installed and models are downloaded.
* **Supported Model Formats:** Not explicitly documented, as Llamao doesn’t let you add custom models. It likely uses optimized quantized models internally (possibly variants of Llama or Qwen tuned for mobile). The focus is on being an “offline ChatGPT alternative”, so the models are chosen to balance size and capability. The app might use something like a 7B model for the free tier, and larger (13B+) for premium if the device can handle it – but exact details are proprietary.
* **Qwen3 14B Support:** **Unknown (unlikely direct).** Llamao’s marketing doesn’t mention specific model names. They do mention models from “Meta, Google, Alibaba, etc.” in some discussions, so it’s possible a Qwen-based model is included. However, as a user you cannot explicitly load Qwen3 yourself. If the developers integrate Qwen3-14B in a future update, it would come as part of the app’s offerings (probably for high-end devices only). Currently, assume it supports smaller Qwen variants at best.
* **API Access:** **No.** Llamao does not provide any API. It’s a closed app aimed at end-users. It prioritizes a simple chat experience on the device itself. For use as a network node, this app won’t be suitable (unless the developers add such a feature down the line).
* **Resource Requirements:** **Moderate.** Because Llamao targets mainstream phones, it is optimized for speed. It likely uses 4-bit quantization and possibly only loads parts of the model as needed. The performance is tuned for interactive chat – reviews suggest it runs *seamlessly offline* on modern devices. For example, tasks like writing emails or answering questions on-the-go are within its intended use. That said, pushing it to do long-form generation or heavy reasoning might hit limits of whatever model it bundles. Battery usage is significant but comparable to other AI apps when engaged in generation.
* **Stability & Support:** **Polished but closed.** Llamao is positioned as a privacy-friendly ChatGPT alternative, and it appears to have a professional development team. The app has been featured on Product Hunt and elsewhere, implying a decent user base. However, being closed-source, you rely on the company (Llamao team) for updates and fixes. So far, feedback indicates it works well for its intended offline use, but flexibility is limited compared to open-source apps.

### **Local AI Chatbot by Software Tailor** (Android app)

* **Setup:** Install from Play Store (app name: *Local AI Chatbot – Offline AI*). It’s a ready-to-go solution; upon launch you can choose from a couple of built-in model options. The description notes “dual model support,” which suggests the app may come with two model modes (perhaps one smaller, one larger) that you can switch between. No additional downloads are needed beyond what the app provides.
* **Supported Model Formats:** Not user-configurable. The app includes **pre-quantized models** such as **DeepSeek R1**, **Qwen**, **Mistral**, **Llama 3**, and **Phi**. These are likely smaller versions (e.g. DeepSeek R1 is 1.5B, Mistral 7B, etc.). The “dual model” feature may let the app load two at once or quickly swap (for example, a fast 1.5B model for simple queries and a 7B model for more complex ones).
* **Qwen3 14B Support:** **Not directly.** The app advertises **Qwen** support, but given the context, it’s probably referring to a lighter Qwen model (for instance, Qwen-7B or Qwen-1.8B). Running a full 14B Qwen3 in this app is not feasible due to memory limits and lack of user-provided model capability. So while you can “chat with Qwen” on this app, it’s not the 14B powerhouse version, but a trimmed-down one suitable for mobile.
* **API Access:** **No.** This is a standalone chatbot app with a simple UI. It doesn’t expose an API endpoint. It’s designed for privacy and offline use on the device itself – *“all processing happening on-device… no data sent out”* – but not for serving other clients.
* **Resource Requirements:** **Low/Moderate.** By limiting to smaller models and optimizing them, this app claims *“quick response times”* and *“minimal resource consumption”* on mobile. It should run comfortably on a Pixel 9 Pro, as it’s also targeting mid-range phones. You might see 5–15 tokens/sec generation speed depending on the model chosen. Memory usage is within a few GB. This makes it very usable for daily offline queries, though at the cost of the sheer accuracy or depth that larger models like 14B would offer.
* **Stability & Support:** **Unknown (new app).** With only a few hundred downloads, this app is relatively new or niche. The developer (Software Tailor) provides the basic documentation on the Play Store listing. It likely uses established libraries under the hood (maybe llama.cpp as well, given similar model lineup) so it should be stable for what it does. Community support is minimal at this stage – it’s more of an off-the-shelf solution than a community project. For straightforward private chat needs, it’s an option, but for tinkering or extending, there’s not much information available beyond the provided feature list.

### **Layla AI** (Android app, freemium)

* **Setup:** Layla is available on the Play Store in two versions – a free Lite version and a paid full version (one-time purchase). You can also download the APK directly from the official site. Once installed, the app comes with its AI persona “Layla” ready to use offline. No additional model setup is needed (nor allowed).
* **Supported Model Formats:** Proprietary/unknown. Layla positions itself as *“the world’s first private AI”* on your phone. Under the hood it likely runs a medium-sized uncensored model (the developers mention no censorship and multiple personalities). They also advertise an image generation feature, suggesting an integrated small Stable Diffusion model aside from the language model. Given Gemma models are a Google release focused on mobile, Layla may be using something like Gemma 4B or Llama2 7B uncensored. The exact model isn’t stated openly.
* **Qwen3 14B Support:** **No.** Layla does not list Qwen or allow custom models. Its focus is on *personas* and role-play (you can download community-made characters/personalities within the app). The model running behind is fixed by the app. It’s optimized more for creative chatting and roleplay than for following strict instructions or coding. Don’t expect it to utilize Qwen3 or any specific large model; it runs whatever the developers have built in.
* **API Access:** **No.** Layla has no API functionality. It’s an entirely on-device, self-contained assistant. It emphasizes privacy (“runs on your device, nothing goes out”) and user freedom (“no external censorship”) but not interoperability. It’s aimed at end-users who want a personal AI companion.
* **Resource Requirements:** **Moderate.** Layla’s target is to run on typical consumer devices without internet. Users report it works on phones with 6–8 GB RAM, so the model is likely in the few-billion parameter range. The Pixel 9 Pro can easily handle it performance-wise. Running the AI for extended chats or image generation will use significant CPU/GPU on the device, so expect warm temperatures. The design philosophy is to give acceptable AI responses within a few seconds on a phone – and user feedback suggests it achieves that for its use cases.
* **Stability & Support:** **Moderate.** Layla is a commercial app with an active website and presumably support channels for customers. Being closed-source, you rely on the company’s updates for improvements. They seem dedicated to the concept of offline AI for everyone, which is encouraging. The app has been updated to include new features (like image generation) as models improve. It’s stable for casual use; however, if your needs extend beyond chat/creative tasks (e.g. serving API calls or doing complex data analysis), Layla’s closed ecosystem will be a limitation.

## **Comparison and Recommendations**

If your goal is to turn the Pixel 9 Pro into a **local AI inference node accessible to other devices**, the **Termux-based servers** are the way to go. Tools like **Ollama** and **LocalAI** give you a managed local API (with model management in Ollama, or OpenAI-compatible endpoints in LocalAI) – ideal for integration with your other systems. **Llama.cpp** is the most bare-bones but also the most battle-tested for running large models on CPU; its new HTTP server mode makes it a strong choice if you enjoy fine-tuning configurations yourself. These will let your Pixel act like a mini self-hosted ChatGPT on your network (just be mindful of the speed and memory limits when using 14B models).

The **Android apps** are more for personal mobile use. Among them, **PocketPal AI** and **ChatterUI** stand out for flexibility and open-source community support. They are great for offline AI interaction on the phone itself, but would require additional work to be used over a network (and might not handle sustained multi-device querying well). Apps like **Llamao, Local AI Chatbot,** or **Layla** are user-friendly and stable for one-on-one chat, but they’re not designed to be servers or to run very large models.

In summary, to run Qwen3-14B on a Pixel 9 Pro and expose it over LAN, your best bet is a Termux solution (Ollama for ease-of-use, or llama.cpp/LocalAI for a more customizable OpenAI-style API). They are offline, privacy-preserving, and have growing communities backing them. The mobile apps can complement this – for instance, testing prompts on-device or having a quick chat – but for an **AI inference node**, go with the Termux server approach for maximum control and integration.
