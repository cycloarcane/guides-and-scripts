# Scraping and API Access for Social Media OSINT (2025)

## Introduction

Open-Source Intelligence (OSINT) investigators often need to gather **live user data** – such as profile bios, posts, comments, followers, and other social footprints – from various social media platforms. This document provides a detailed overview of how to access such data on major platforms via both **official APIs** and **unofficial or reverse-engineered methods**. For each platform, we outline available API access (with authentication requirements, rate limits, and pricing), and alternative scraping tools or libraries that can retrieve real-time data without official API use. We focus on free-to-use tools and techniques suitable for OSINT, and note the **data types** obtainable (e.g. user bios, posts, comments) and the **freshness** of data (real-time vs cached). **Current (2025) statuses** of these methods are highlighted, since API policies and scraping effectiveness can change rapidly.

Below, we cover **Twitter/X**, **TikTok**, **Facebook**, **Instagram**, **Reddit**, **LinkedIn**, **YouTube**, **Telegram**, and **Mastodon**, each in separate sections. Summary tables are included for each platform to compare official API options with unofficial scraping tools in terms of capabilities and limitations.

---

## Twitter / X Platform

Twitter (rebranded as **X**) has historically been a rich OSINT source due to its public posts and profiles. However, policy changes in 2023–2024 significantly altered API access. Below we examine official API avenues and unofficial scraping methods for Twitter.

### Official API Access (Twitter/X)

Twitter’s official API (v2) allows programmatic access to tweets, user profiles, and more – but as of 2023, **free access for data reading was effectively removed**. Twitter introduced paid tiers: the **Basic** tier costs \$100/month and permits retrieval of only **10,000 tweets per month**, while higher tiers (e.g. **Pro \$5,000/mo** for 2 million tweets) target enterprise needs. The only *free* API functionality is limited to posting new tweets (write-only) and media upload, with no free endpoints for reading user data or tweets. This means OSINT practitioners cannot rely on the official API for user information unless they pay for a tier or have special research access.

**Data and Endpoints:** The v2 API (with appropriate tier) provides endpoints for user profile lookup (returning bio, account creation date, etc.), user tweet timelines, tweet search, and followers/following lists. For example, a paid Basic app could fetch a user’s recent tweets or search tweets by keyword. Rate limits are strict – Twitter enforces app-level and user-level caps (e.g. a Basic app maxes out at 10k tweets/month as noted). In addition, **OAuth 2.0 authentication** with a developer project is required for all endpoints. There is currently **no official free tier for reading data**, so the official API is largely a paid service as of 2025.

**Practical OSINT Impact:** Given the low allowance and cost, the official API is often impractical for OSINT (especially large-scale or long-term monitoring). Some academic researchers can apply for special access, but that is outside normal use. For small-scale needs (e.g. checking a few profiles or tweets), one might use a Basic account if already subscribed. Otherwise, OSINT investigators turn to web scraping solutions.

### Unofficial and Scraping Methods (Twitter/X)

**Web Scraping & Reverse-Engineered APIs:** After free API access was cut off, many turned to scraping Twitter’s web frontend. Twitter’s web app uses **public GraphQL/REST endpoints** (with a guest token mechanism) to load tweets and profiles. By mimicking these requests, scrapers can fetch tweets and user info without official API keys. This often requires **headless browser automation** or advanced HTTP clients due to Twitter’s anti-scraping measures. For example, a headless browser can capture the background API calls when loading a profile or tweet page (calls like `UserBy` or tweet data queries). Using tools like **Playwright** or specialized services (e.g. Scrapfly), scrapers intercept these XHR calls to retrieve JSON data for tweets or user profiles. This approach can yield a user’s bio, follower count, and recent tweets in **real time**, effectively bypassing official API restrictions. However, since mid-2023 Twitter requires login for most content, completely **unauthenticated scraping is limited**. Many scrapers now either use a logged-in session cookie or Twitter’s temporary guest tokens to access data. This is a cat-and-mouse game: changes in Twitter’s frontend can break scrapers.

**Notable Tools:**

* *Tweepy* – An official Python library for Twitter API (requires API keys). Useful if you have API access; otherwise not applicable under new restrictions.
* *Twint* – A popular Python OSINT tool that **scrapes Twitter without using the API**. Twint can gather a user’s tweets, followers, following, and bio entirely via web requests, avoiding rate limits. **Status:** As of 2025, Twint’s repository is archived and it no longer receives updates. Twitter’s 2023 login requirements have partially broken Twint’s functionality (e.g. tweet search) unless a workaround is implemented. Some investigators still use Twint for basic profile scraping, but it may fail or require fixes due to site changes.
* *snscrape* – An open-source scraper for social networks that supported Twitter scraping (for tweets, user profiles, etc.). It worked by using Twitter’s public search endpoints. After Twitter began forcing login for searches in 2023, snscrape’s capabilities were curtailed – it can still retrieve individual tweets or limited data, but **full user or keyword scraping may not work at scale** without authentication. Developers note that snscrape is not reliable for large Twitter scraping post-2023.
* *Browser Automation:* Tools like Selenium or Playwright can script a headless browser to log in to Twitter and navigate to a target profile or search results, then parse the content. This can reveal **bios, tweets, replies**, etc. The **Scrapfly** project demonstrated this approach, capturing JSON from Twitter’s internal API calls to get tweet and profile data. While effective, this method is resource-intensive and may require solving occasional anti-bot challenges.

In summary, **unofficial methods are currently the primary way to gather live Twitter data for OSINT**, given the stringent limits of the official API. Investigators should use these with caution: abide by Twitter’s Terms of Service, use rate limiting to avoid detection, and be prepared for breakages as the platform evolves.

**Twitter Data Accessible:** Using a combination of scraping tools, OSINT investigators can obtain *user profile details* (bio, profile image URL, join date, etc.), *recent tweets from a user*, *replies to a user’s tweets (comments)*, and even a list of followers/following (Twint historically could pull these). Data is as fresh as the moment of scraping (real-time), since these methods fetch directly from Twitter’s live site. Table 1 compares Twitter API access vs. scraping tools:

**Table 1 – Twitter/X Data Access Options (Official API vs Unofficial Tools)**

| Method                                                  | Type                      | Data Accessible                                                                                                  | Auth Requirements                                                                                     | Limitations & Notes                                                                                                                                                          | Status (2025)                                                                                                                                                                                  |
| ------------------------------------------------------- | ------------------------- | ---------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Twitter API v2** (Official) – e.g. via Tweepy library | Official API              | User profile info (bio, name, etc), tweets, follower lists, search queries (depending on endpoint and tier)      | **API Key & OAuth2** (must have Twitter developer app; paid tier needed for reads)                    | Free tier **write-only** (no read access). Basic \$100/mo for 10k tweet reads; strict rate limits per app. Requires compliance with Twitter policies.                        | **Active** but costly. No free data reads; hobbyist projects largely priced out.                                                                                                               |
| **Twint** (Python scraper)                              | Unofficial (web scrape)   | Tweets from user timelines, user bios, follower/following lists, tweet search results, etc.                      | **No login required** (scrapes public web requests; can use proxies)                                  | Not an official API – subject to webpage structure changes. No guaranteed support; Twint is **archived** by author. May break if Twitter modifies HTML or requires login.    | **Partially working.** In 2025, core functions often need fixes due to X’s anti-scraping changes. Useful for basic scraping if updated, but reliability is **not guaranteed**.                 |
| **snscrape** (Python)                                   | Unofficial (web scrape)   | Twitter user profiles and tweets (historically via search queries or user timeline)                              | **No login for limited use** (fetches public data; may need cookies for extensive use)                | Open-source scraper that avoids API quotas. However, since mid-2023, Twitter’s login wall blocks unauthenticated searches – significantly limiting snscrape’s effectiveness. | **Limited functionality.** Can retrieve individual tweets or small sets. Bulk scraping (e.g. full user history or keyword search) is **hampered by login requirements** and Cloudflare blocks. |
| **Headless browser scraping** (Playwright/Selenium)     | Unofficial (web automate) | Any content visible on the Twitter website: full user timelines, tweet threads (comments), profile details, etc. | **Requires a Twitter account** (to log in via the automated browser) or use of temporary guest tokens | Bypasses API limits by simulating a real user. High resource usage. Might trigger bot detection or require solving CAPTCHAs. Needs maintenance when site UI or calls change. | **Effective but heavy.** Used in advanced OSINT; yields real-time data. Maintenance burden is high due to frequent site changes by X.                                                          |

*Table 1: Comparison of Twitter/X official API vs unofficial scraping tools.*

---

## TikTok

TikTok presents unique challenges for OSINT data collection. It lacks a widely available public API for general user data, but does offer specialized APIs for certain partners. OSINT investigators often rely on unofficial **reverse-engineered APIs** or scraping methods to get TikTok user info and content.

### Official API Access (TikTok)

TikTok’s official developer offerings are limited in scope and accessibility:

* **TikTok Developer APIs:** TikTok provides APIs primarily for embedding videos, social login, and for advertising/marketing (TikTok for Business API). These do *not* give broad access to user data or posts on demand; they are meant for creators or advertisers to manage their own content. There is also a **Basic Display API** for developers to pull **their own account’s videos** and profile info after user authentication, but not for arbitrary users.

* **TikTok Research API:** Introduced to support academic researchers, this API allows querying public TikTok data. It is **not openly accessible** – one must apply and be **approved as a research project** to receive credentials. If granted, the Research API can deliver robust data: endpoints exist to **query user info, followers, following, video posts, comments, likes, etc** for public accounts. For example, there are endpoints to get a user’s profile details, their followers list, or comments on their videos. The Research API supports complex queries (with filters similar to SQL) to find content across TikTok. However, each query requires a valid **client key/secret and OAuth token** from the approved project. Rate limits and data caps for this API are not publicly documented, but access is likely **free for approved researchers**, subject to usage agreements. In summary, unless you’re in a special program, the official API is not an option for everyday OSINT use.

* **TikTok API for Business:** This is intended for advertisers and partners to manage ads, retrieve analytics, and perhaps moderate comments on their own videos. It does not provide endpoints to scrape arbitrary user profiles or videos by other users. It’s also gated (requires applying as a TikTok Business partner). Pricing is not public; presumably it’s part of enterprise agreements.

Given these constraints, OSINT practitioners rarely rely on TikTok’s official APIs. The most relevant official access – the Research API – is powerful but exclusive (academic use only). No general free or low-cost API exists for pulling public TikTok data at will.

### Unofficial and Scraping Methods (TikTok)

**Reverse-Engineered APIs:** TikTok’s mobile and web apps communicate with private endpoints that have been reverse-engineered by the community. One popular tool is the **Unofficial TikTok API wrapper for Python** (often referred to as **TikTokApi** library). This library, by David Teather, allows retrieval of public TikTok data by emulating TikTok’s own app requests. Using this or similar tools, OSINT investigators can collect:

* **User profile information:** username, profile bio, follower count, following count, total likes, etc.
* **User videos:** a list of recent videos posted by a user, including metadata (captions, view counts, etc.).
* **Trending or hashtag videos:** the library can pull currently trending videos or videos under a specific hashtag, as exposed by TikTok’s public feeds.

Notably, the unofficial API **cannot access private content or perform user actions** (it’s read-only for public data). It also **cannot post videos** or authenticate as a specific user – it works by simulating a logged-out client, so any data that requires login (e.g. seeing a private account’s posts, or a user’s full following list if hidden) is unavailable. In other words, “if you can’t access it while being logged out on their website, you can’t access it here”. The advantage is that no official API key or login is needed; the tool handles the necessary tokens or signatures internally.

**Technical considerations:** TikTok’s web endpoints often require solving anti-scraping challenges. TikTokApi (Python) historically included methods to obtain the required request signatures or cookies (sometimes by integrating with a browser or external service to solve captchas). Usage of these unofficial APIs is generally free (open-source), but one must keep the library updated. The maintainer notes the library “will always be free and open-source”. TikTok does employ countermeasures (rate-limiting IPs, requiring JavaScript-based tokens). Therefore, heavy scraping might necessitate rotating proxies or periodic human intervention to pass anti-bot checks.

**Alternate Methods:**

* *Headless Browser Scraping:* Similar to Twitter, one can use Selenium or Playwright to load TikTok’s web pages (e.g. a user’s profile page) and parse out data. However, TikTok heavily relies on JavaScript – the profile page makes XHR calls to load videos, which a headless scraper can capture. This method can retrieve video metadata and even some **comments**. Note that viewing comments on TikTok’s web player often requires clicking and may only load a limited batch without login. Some OSINT practitioners use browser automation to scroll comment sections and collect text of comments.
* *Mobile API Emulation:* Advanced scrapers try to emulate TikTok’s Android or iOS app API. This can yield additional info (like **full comment threads**, or **user liked videos** if not protected). But TikTok’s app API is locked down with encryption and constantly changing parameters (like `X-Gorgon` or `X-Ladon` tokens). The open-source community sometimes figures these out; more often, paid services (like **TikAPI** or other commercial APIs) provide this data. Since we focus on free tools, we acknowledge these exist but they typically cost money or violate TikTok’s terms.

**Data obtainable:** With the unofficial approaches, OSINT can gather *public user bios and stats*, *lists of a user’s videos (including captions and counts)*, *thumbnail or video links*, and *top comments on those videos*. **Follower and following lists** are more difficult – TikTok doesn’t show full lists publicly (only counts), so scraping those would require login and is not straightforward even with the library. The **freshness** of data is real-time – when you make a request, you get the latest profile counts and video data directly from TikTok’s servers.

One must remember that TikTok user content can be highly dynamic (counts updating quickly, videos sometimes removed). Scrapers should be run close to the needed time to ensure data is up to date.

**Table 2 – TikTok Data Access Options** summarizes official vs unofficial methods:

**Table 2 – TikTok API vs Unofficial Access**

| Method                                                 | Type                            | Data Accessible                                                                                                                 | Auth Requirements                                                                                                                                                   | Limitations & Notes                                                                                                                                                                | Status (2025)                                                                                                                                               |
| ------------------------------------------------------ | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **TikTok Research API** (Official)                     | Official API (restricted)       | Public account data: user profile info, video metadata, followers/following lists, comments, etc – designed for broad queries.  | **Researcher Application & Approval** (requires TikTok to grant access; uses OAuth token)                                                                           | Only for approved academic projects. Not commercial. Detailed querying with filters possible. No direct cost, but gated and monitored usage.                                       | **Active (limited)** – Powerful if you can get it, but *not available to most users*. Use limited to research purposes.                                     |
| **TikTok Basic/Business APIs** (Official)              | Official API (dev kit)          | Minimal data for integration: your own account’s videos, or advertising analytics. *No endpoint for arbitrary user content.*    | **Developer App & Token** (must register app on TikTok Dev Portal)                                                                                                  | Useful for login kits, video upload, or embedding content. Cannot pull other users’ posts freely. Business API focuses on ads/metrics.                                             | **Active** – but *irrelevant for OSINT* data gathering since they don’t provide general user info.                                                          |
| **TikTokApi (Python)** – Unofficial TikTok API wrapper | Unofficial (reverse-engineered) | Public profile info (bio, stats), user’s **public videos** (titles, URLs, counts), trending feeds, hashtag search results, etc. | **None** (no login or key; uses internal web API calls)                                                                                                             | Open-source library simulating TikTok’s web requests. No private data or actions (read-only). May require solving TikTok’s anti-bot challenges; needs updates when TikTok changes. | **Active** – widely used for OSINT. As of 2025, still functional for most public data. Requires maintenance to handle occasional TikTok countermeasures.    |
| **Headless Browser Scraping** (Playwright/Selenium)    | Unofficial (web scrape)         | Any data visible on TikTok’s web interface: profile page info, video lists, partial comments, etc.                              | **Optional** – Many profiles and videos can be accessed without login (for public users). For expanded comments or certain actions, a TikTok login might be needed. | Bypasses API by parsing the live site. Subject to UI changes and heavy use of resources. TikTok’s dynamic loading and possible CAPTCHA challenges can hinder this method.          | **Active** – a viable fallback if APIs fail. Often used to grab comments or verify data from the TikTokApi library. Requires keeping a browser environment. |

*Table 2: TikTok data access – official vs unofficial methods.*

---

## Facebook

Facebook is a mature platform with an extensive official API (the **Graph API**), but due to privacy changes since 2018, accessing user data via the official API is highly restricted. OSINT efforts on Facebook often involve creative use of whatever public-facing data can be scraped.

### Official API Access (Facebook Graph API)

Facebook’s Graph API allows programmatic access to Facebook data, but **primarily for content that you control or that is explicitly made public**, and even then under strict conditions. Key points regarding official access:

* **Limited Public Data:** In the wake of data privacy scandals, Facebook severely limited what the Graph API can provide about users. As of 2025, third-party apps *cannot* freely fetch personal profile data or posts of other users. The Graph API’s search functionality was curtailed in 2018 – you can no longer search for people or posts arbitrarily. The only search API available is for **Facebook Pages** (and some limited **Groups**). Access to page content that’s public is possible with the right permissions (see below), but **searching posts by keywords or fetching random user profiles via API is disabled**.

* **Permissions and App Review:** To use Graph API for anything beyond basic public page info, you must create a Facebook App and obtain **access tokens**. There are multiple token types – user access tokens (on behalf of a Facebook user), page tokens (for page admins), app tokens, etc.. Certain data requires specific **permissions** (e.g., `pages_read_user_content` for reading comments on a Page) which in turn require Facebook to **approve your app via a review process**. For example, reading public content from pages not managed by you needs the *Page Public Content Access (PPCA)* permission. This is only granted after demonstrating a legitimate use case (often only to bigger companies or researchers). Similarly, reading group content needs group permissions and is being deprecated for third parties.

* **User Profile Data:** The Graph API can retrieve some basic info of a user *who has authorized your app*. The default “public profile” permission only gives **very limited fields**: name and profile picture, and a few IDs. It **does not** give you posts or friend lists of that user unless additional permissions are granted by Facebook (which are typically not granted for general use). Essentially, you cannot use the official API to grab random users’ bios or posts. Only a user themselves can consent to share their data with your app, or you pull data from assets you manage (pages, etc.).

* **Pages and Groups:** If investigating a **Facebook Page** (e.g., a company or public figure’s page), the Graph API can be useful. Public pages’ posts and comments *can* be accessed through the API, but again PPCA permission is needed if the page isn’t yours. There are some exemptions: certain public page data (name, description, recent posts) might be accessible with just an app token for unrestricted pages. But anything beyond that (like the list of page followers, or comments on posts) needs elevated permissions. Facebook *Groups* API access is very limited now; Facebook announced deprecation of some group endpoints for third parties, and only group admins using special apps can pull group content.

* **Rate Limits and Pricing:** Facebook’s Graph API is free to use in terms of cost – Facebook does not charge for API calls. Instead, they manage usage via **rate limiting**. For example, the Graph API might allow around 200 calls per hour per user token by default (exact numbers vary by endpoint). They also have bursts limits like 600 calls per 600 seconds in some cases. If you approach limits, the API returns a 429 error. Most OSINT use cases that do get authorized access will typically be low volume, so hitting limits is rare. There is no paid tier; if you need more, you request higher limits from Facebook. But importantly, the barrier is not the rate limit – it’s the data permissions.

**Summary:** Official API use in OSINT is usually confined to page data or one’s own data. For instance, using Graph API you might gather all posts from a propaganda Facebook Page (if approved for PPCA) along with comments. But you cannot use it to enumerate posts on a person’s profile who hasn’t authorized your app. **Most personal profile OSINT on Facebook must rely on scraping the front-end**, due to these Graph API lockdowns.

### Unofficial and Scraping Methods (Facebook)

**Direct Web Scraping:** The primary way to gather Facebook data for OSINT now is by scraping the website as a normal user. This often means automating a **logged-in session**, because Facebook shows very little content to logged-out users. A few techniques and tools:

* **facebook-scraper (Python library):** An open-source tool that scrapes Facebook **public pages and groups** without using the official API. It works by sending HTTP requests to Facebook’s mobile or basic web version and parsing the HTML. With `facebook-scraper`, one can retrieve Page posts (including text, timestamps, reaction counts) and even some **Group posts** or **Profile posts** if they are public. It supports fetching multiple pages of posts and can also log in with provided credentials to access content behind a login wall. For example, you can do: `get_posts("nintendo", pages=1)` to get recent posts from the Nintendo Facebook page. The library will handle things like pagination and can optionally load comments, though by default it might fetch only a subset of comments or just counts. It’s a valuable OSINT tool for Facebook Pages especially. **Limitations:** It might not scrape personal profiles reliably (as most profiles are not fully public). It can scrape **Groups** if they’re public or if you supply a login that’s a member of the group (using `credentials` or session cookies). It doesn’t use official API keys, so it’s free, but one must be mindful of Facebook detecting unusual scraping activity (using a low volume and possibly throwaway accounts helps).

* **Manual/Custom Scraping:** Many investigators use **Selenium or Playwright** to automate a browser logged into a sock-puppet Facebook account. This allows them to navigate to a target’s profile, scroll their timeline, and collect whatever is visible (posts that are public or visible to “Friends of Friends” if the sock account qualifies, etc.). They may also access sections like “About” to gather a user’s bio, workplace, education info (if not private), and the friends or followers list (if public). **Important:** Facebook’s interface is dynamic and heavy; scraping it requires handling infinite scroll regions, and content may load slowly or be missing if not scrolled into view. Some OSINT techniques involve switching to Facebook’s **“mbasic”** mobile web interface, which is a simplified HTML version of Facebook. The mbasic.facebook.com site often works better for scraping as it’s less reliant on JavaScript and splits content into pages. For example, one can fetch `mbasic.facebook.com/<profile_id>/friends` to parse a list of friends (if visible) or `mbasic.facebook.com/search/posts/?q=<keyword>` for searching public posts by keyword (though this is very limited now).

* **Reverse-Engineered GraphQL (Web endpoints):** Facebook’s web app internally uses GraphQL calls for certain features (like loading more comments). In theory, if you inspect network calls, you can find endpoints that return JSON of posts or comments. However, these almost always require a valid user token (the `fb_dtsg` and cookies from a logged session) and are tied to that session’s privileges. Some advanced scrapers utilize these to directly request data (e.g., fetch all comments for a post in JSON instead of parsing HTML). This approach is complex and tightly coupled to the app’s internal schema, which can change at any time.

**Data Accessible via Scraping:** If a target user’s privacy settings are open enough (or you have a friend connection), you can gather a trove of info by scraping: profile bio/about info, profile picture, cover photo, list of friends or followers (unless hidden), posts they made public (including text, media, timestamp, and reactions count), and comments on those posts. You can also search for the user’s name across Facebook to find any public mentions or posts where they are discussed (the Graph Search was removed, but you can use the standard search interface with some filters manually or via scraping). **Comments** they made on others’ content are harder – you’d have to find the specific posts (perhaps by known associations or using search if available) and then see if their name appears in the comments.

**Freshness:** Facebook data can be scraped in real-time. For example, using a logged-in bot, one could monitor a target’s profile periodically to detect new posts or new friends. The limitation is if the target’s settings restrict visibility, no scraping can overcome that without the scraper account being appropriately permissioned (friend or group member).

**Operational Security:** It’s worth noting that scraping Facebook carries some risk – Facebook actively fights unauthorized automation. Accounts used for scraping can get flagged for unusual activity (lots of page loads, rapid browsing). Investigators often use **dedicated burner accounts** and go slow. There are also third-party services (like PhantomBuster or Scrapin) which offer cloud-based Facebook scrapers that handle some of this complexity (but those may cost money).

Below is a comparison table summarizing Facebook official vs scraping methods:

**Table 3 – Facebook API vs Scraping Options**

| Method                                                  | Type                         | Data Accessible                                                                                                                                                                                 | Auth Requirements                                                                                                                                                                         | Limitations & Notes                                                                                                                                                                                                            | Status (2025)                                                                                                                                                                           |
| ------------------------------------------------------- | ---------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Facebook Graph API** (Official)                       | Official API                 | Public Page data (ID, name, posts, comments) with permissions; Basic user profile fields (name, pic) if user authorized app. *Virtually no access to non-page content without user permission.* | **Facebook App & Token**. Many data fields need special permissions (PPCA for pages, user\_\* for profiles) and app review. User must auth for profile data.                              | **Highly restricted**: Cannot retrieve arbitrary user posts or friends. Only practical for page monitoring or your own data. App approval process is hard.                                                                     | **Active** – used in niche cases (e.g. academic studies on pages). For general OSINT on profiles, official API is *mostly unusable*.                                                    |
| **facebook-scraper (Python)**                           | Unofficial (HTML scrape)     | Public Page posts (text, timestamps, reactions counts), some Comments, Public Group posts, and sometimes Profile posts if public. Can get basic info from profiles (name, intro) if visible.    | **Optional Login** – Can work with no login for pages/groups that are public. For profiles or closed groups, needs Facebook credentials (which library can use via cookies or user/pass). | Free and no API key needed. Parsing HTML is brittle if FB changes layout. Often uses mobile web endpoints for simplicity. May not get all comments or media without extra steps.                                               | **Active** – Maintained and widely used for scraping Facebook pages/groups. As of 2025, still effective for publicly available content. Struggles with content behind privacy settings. |
| **Browser Automation (Selenium/Playwright)**            | Unofficial (web automate)    | Any content the logged-in account can see: target’s profile info, posts, friends list, comments in groups, etc.                                                                                 | **Facebook Account Login** (a sock-puppet user with appropriate connections or group memberships)                                                                                         | Can capture dynamic content (stories, loaded comments) that static scrapers might miss. But very resource-intensive and can trigger anti-bot measures (login checks, locks). Requires careful human-like interaction patterns. | **Active** – Often the last resort for complex scraping (e.g. scrolling through years of timeline). Effective but maintenance-heavy.                                                    |
| **Other 3rd-Party APIs** (e.g. Data365, PeopleDataLabs) | Unofficial (commercial APIs) | Varies – e.g., databases of public Facebook profile info, or APIs that provide page post data without Graph permissions.                                                                        | **None for user** (they handle data collection)                                                                                                                                           | These services aggregate data (some scrape, some use private agreements). Often not free (commercial). Data might not be real-time (cached profiles updated every X days).                                                     | **Varies** – Outside the scope of pure OSINT tools, but options exist. Often used when direct scraping is too challenging, albeit at cost.                                              |

*Table 3: Facebook data access via official Graph API vs unofficial scraping tools.*

---

## Instagram

Instagram, owned by Meta, also has an official API with tight restrictions and is a common target for OSINT scraping. Instagram’s web interface and mobile API have been reverse-engineered extensively, producing robust tools for data extraction.

### Official API Access (Instagram Graph API)

Instagram’s official APIs are part of the Facebook Graph API and are focused on **business and creator accounts**, with very limited personal data access:

* **Instagram Graph API (for Business/Creator Accounts):** This API allows a business or creator to access their own Instagram account data (posts, insights, comments on their media) and perform actions like posting. Crucially, it also provides an endpoint called **Business Discovery** which lets you fetch basic profile info and recent posts of *other Instagram Business or Creator accounts*. This means if the target account is a professional account (many influencers, brands, and public figures use business accounts), and you have a Facebook app with the right permission, you can query that account’s profile (username, biography, follower count, etc.) and a list of their recent posts (with captions, like count, comment count) via the API. However, *personal* Instagram accounts (the default for private individuals) cannot be accessed this way. The Graph API explicitly limits data to business/creator accounts and requires your app’s Instagram business account to “discover” them by username. You must have the **instagram\_graph\_user\_profile** and **instagram\_graph\_user\_media** permissions, and the target account must not be private.

* **Instagram Basic Display API:** This is a simpler OAuth API that lets an app fetch *a user’s own* Instagram media, after that user logs in and consents. It’s useful for allowing users to import their photos to another app, for instance. It cannot pull data about other users – only the authenticated user’s profile and media. Not useful for OSINT on a third party, except in scenarios where the target themselves gives access (unlikely).

* **Rate Limits & Access:** Both the Graph API and Basic API require a Facebook Developer app, which is free but needs to go through **app review** for most permissions. There is no monetary cost, but significant friction in getting approval. Once approved, rate limits are fairly generous for normal use (e.g., 200 API calls per hour per token, often more). The Business Discovery endpoint returns up to 50 posts of the target per query. If you needed to page through more historical posts, you’d make additional requests (each counted against limits). There is no free/open “explore Instagram” API – everything is permission-gated.

**Summary:** Officially, you can only retrieve Instagram data under very specific conditions (e.g. target is a business profile). This typically doesn’t cover most individuals. Therefore, OSINT practitioners lean heavily on unofficial methods for Instagram.

### Unofficial and Scraping Methods (Instagram)

Instagram’s website and private API have been extensively leveraged for scraping. Some key tools and techniques:

* **Instaloader (Python):** This is an open-source tool specifically built for Instagram scraping. It can **download pictures or videos along with captions and other metadata from Instagram**. Importantly for OSINT, Instaloader can retrieve a wealth of data:

  * Public **profile information**: username, full name, bio (profile description), profile picture, number of posts, follower count, following count.
  * **Posts**: All posts from a target profile (if the profile is public, or if private and you have login credentials that follow them). Instaloader can download each post’s image/video, caption text, hashtags, timestamp, and even **location tags** and **geotags**.
  * **Comments**: If the `--comments` option is used, Instaloader will fetch the comments on each post as well. This means you can scrape what people are commenting on the target’s posts (useful for sentiment or leads).
  * **Stories and Highlights**: Instaloader, when logged in, can save the user’s current Stories and their Highlights if the account is accessible.
  * **Followers/Following lists**: Instaloader can collect the list of usernames of followers and followings, *but* this requires authentication and is subject to Instagram’s limitations (Instagram may not allow querying the entire list if it’s very large, or may throttle this heavily).
  * **Saved posts and tagged posts**: The tool also can scrape posts where the user is tagged, and the user’s saved posts (if you have their account credentials), though those are less relevant for external OSINT.

  Instaloader works by making requests to Instagram’s **public web API endpoints** (which return JSON) and GraphQL endpoints. It can operate in **logged-out mode for public data**, but Instagram now blocks unauthenticated access after a short while or for content beyond a few pages. Therefore, it’s often necessary to use `--login=your_username` with a throwaway Instagram account to scrape extensively. The tool handles the login and stores session cookies to avoid re-login each time. Logged in, it behaves like a normal user’s browser fetching data, which increases the amount of data you can get before hitting walls.

  Instaloader is quite robust and actively maintained; it automatically adapts to some site changes. It **respects rate limits** by waiting between requests, but if you scrape too fast or too much, Instagram might temporarily ban the account or show a “suspicious login” prompt. Using realistic delays and not scraping millions of profiles in one go is advised.

* **Instagram Private API libraries (e.g. instagrapi):** These libraries imitate the official Instagram app’s API. By logging in via a throwaway account, they can retrieve data in JSON form. For example, the `instagrapi` Python library can fetch a user’s info by username, get their posts, fetch the comments for a post, etc., by replicating mobile requests. Such libraries may access certain endpoints Instaloader doesn’t (like viewing who liked a post, or querying a user’s activity feed), but they require more care (they often need to handle challenge verifications from Instagram). They also allow certain actions (like sending DMs, which OSINT might not need, but indicates broad capability).

* **Direct HTML Scraping:** In simpler cases, one can scrape the Instagram web profile pages. For instance, visiting `https://www.instagram.com/<username>/` returns an HTML page that in its source contains a JSON payload with the profile’s info and the first batch of posts (this is the `__a=1` endpoint, which as of 2023 requires login). By parsing that, you get similar data as Instaloader would. However, Instagram’s web is built to load more posts via AJAX calls as you scroll, so a static HTML fetch won’t get everything – that’s why using a tool or writing a script to page through (or using the GraphQL endpoints for pagination) is needed.

**Data & Freshness:** Unofficial methods can capture essentially all **publicly visible data** on Instagram. If a profile is public, you can get *all their past posts* (subject to time and rate limits – but Instaloader can resume and continue until done) along with captions and comments. That means you could reconstruct a timeline of their activity. Followers and following lists can also be gathered (though if the user has, say, 10k followers, scraping that entire list will be slow and may trigger rate limits). The data is live/current – you get what’s on Instagram at that moment, including updated follower counts or the latest posts up to the minute.

One limitation: if a target’s profile is **private**, none of this data can be accessed unless the scraping account is an approved follower of that private account. OSINT typically respects that boundary (or attempts to become a follower using a sock puppet, which goes beyond just scraping).

**Table 4 – Instagram Data Access Comparison**:

**Table 4 – Instagram Official vs Unofficial Data Access**

| Method                                          | Type                        | Data Accessible                                                                                                                                                                                       | Auth Requirements                                                                                                                            | Limitations & Notes                                                                                                                                                                 | Status (2025)                                                                                                                                                    |
| ----------------------------------------------- | --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Instagram Graph API** (Business Discovery)    | Official API (Graph)        | Profile info (username, bio, profile pic URL, follower count) **for Instagram Business/Creator accounts only**; Recent media posts of target (captions, media URLs, engagement metrics).              | **Facebook App + Instagram Business Account**. Requires target to be business/creator and your app to have permission.                       | Cannot access regular personal accounts. No comments via this unless they are on your own media. Strict app review needed.                                                          | **Active** – used by businesses; *OSINT use is rare* (only useful if target is a public figure with a business account).                                         |
| **Instagram Basic Display API**                 | Official API (OAuth)        | A user’s **own** profile info and media library. (Not for other users)                                                                                                                                | **Instagram user login** (the target would need to log in to your app)                                                                       | Not applicable for third-party target data. Meant for user giving access to their data.                                                                                             | **Active** but *not useful for OSINT* unless target cooperates.                                                                                                  |
| **Instaloader (Python)**                        | Unofficial (scrape/web API) | *Public or auth-accessible:* Profile metadata (bio, counts), all posts (images/videos, captions, hashtags, timestamps, geotags), **comments on posts**, Stories, highlights, and even followers list. | **Optional Login** – Strongly recommended to use a login for anything beyond a handful of posts. Uses session cookie to mimic a normal user. | Open-source and free. Handles private accounts only if login is follower. Must respect IG rate limits (can pause/resume scraping). Large scale scraping can trigger temporary bans. | **Active** – Very reliable. As of 2025, continues to adapt to Instagram changes. Widely used in OSINT for Instagram footprinting.                                |
| **instagrapi / Instagram Private API wrappers** | Unofficial (mobile API)     | Similar scope as above: profile info, posts, comments, plus possibly story viewers, etc. Essentially anything the official app can fetch for that account.                                            | **Login required** (via a throwaway IG account API login).                                                                                   | Tends to be faster (retrieves JSON directly) but more brittle on auth – may get challenge or ban if patterns detected. Requires keeping library updated with Instagram app changes. | **Active** – Used in automation projects. Provides an alternative if web-scraping fails. Must handle frequent breakages whenever IG updates their app endpoints. |
| **Manual Web Scraping (Requests/BS4)**          | Unofficial (HTML scrape)    | Public profile page info and initial posts (if not logged in), or more if logged in and using the older JSON endpoints.                                                                               | **Login cookies** if scraping beyond public landing page.                                                                                    | Without using a library like above, custom scrapers need to handle dynamic loading or use undocumented endpoints. Instagram aggressively prompts for login after a few requests.    | **Active** – Custom scripts are possible, but most rely on established tools now.                                                                                |

*Table 4: Instagram official API vs unofficial scraping tools.*

---

## Reddit

Reddit offers one of the more accessible official APIs among social platforms, and it has been widely used in OSINT. However, policy changes in mid-2023 introduced new limitations and pricing for heavy usage. Still, for moderate use, Reddit’s official API remains usable and free. Unofficially, the **Pushshift** service provided historical Reddit data, but its free access has been curtailed recently.

### Official API Access (Reddit)

Reddit’s official API (v1 and v2) is a JSON REST API that covers nearly all functionality of the site: reading posts, comments, user profiles, search, etc. Key details:

* **Open Access (with API registration):** Anyone can register an app on Reddit (for free) and obtain API credentials (client ID and secret). Using OAuth2, you can then obtain a token either as “script” (personal use) or via user login. **Most API endpoints do not require special permission** beyond a valid token. This ease-of-access made Reddit API popular for academic and OSINT projects.

* **Data Available:** The API can fetch:

  * **User profile info:** via `/user/<username>/about.json`, which returns public information about a user (creation date, karma breakdown, and the user’s **“about” text (bio)** if they have one). It does not give private details like email.
  * **User contributions:** Reddit allows listing a user’s posts and comments. `/user/<name>/submitted` gives their posts, and `/user/<name>/comments` gives their comments, in reverse chronological order. This is extremely useful for OSINT – you can pull the entire posting history of a user (up to API limitations). Note that very old posts may eventually not be returned by the API if they are archived, but generally you can get a significant history.
  * **Subreddit content:** You can retrieve posts from subreddits, comments in threads, etc. This might be relevant if investigating communities rather than individuals.
  * **Search:** The API offers search endpoints to find posts or comments by keyword, though the results might not be comprehensive if not using advanced parameters.
  * **Real-time streaming:** There are unofficial means to listen to new comments or posts (via websocket or polling certain endpoints) which some bots use for real-time alerting.

* **Rate Limits:** In 2023, Reddit introduced **100 requests per minute per OAuth client for free** use. Without OAuth (i.e., old “anonymous” use) it’s only 10 requests/minute, so using OAuth is essential. The **free tier** is intended for non-commercial/personal use and is quite generous (100/min equals 6,000/hour). For perspective, retrieving a single user’s 1000 latest comments might take a few API calls with pagination – well within free limits. For heavier usage, Reddit’s new policy is to charge \$0.24 per 1,000 requests beyond the free allowance. However, certain use cases (bots for moderation, accessibility apps) are exempt or can get free continued access. In practice, as of 2024–2025, individual researchers and hobbyists are generally still able to use the API without paying, as long as they stay under the free cap and register their app.

* **Authentication:** You’ll typically use a “script” app type which is tied to your account. That gives you a token that can access any public data. Private data (e.g., your own private messages or a private subreddit’s content) requires that account to have access. For OSINT, we focus on public data, which is widely accessible.

**Recent Changes:** The mid-2023 changes did cause some third-party apps to shut down (because they were making millions of calls and would incur high costs). But if you are doing targeted data gathering (for example, pulling a few thousand posts or comments from a user or subreddit), the official API remains viable and free. The main effect is that if you wanted to *continuously ingest all Reddit posts/comments* (like some projects did), that’s now expensive.

### Unofficial and Historical Data (Pushshift)

For years, OSINT analysts leveraged **Pushshift**, a third-party archive of Reddit. Pushshift collected and stored nearly all Reddit posts and comments in an independent database, accessible via a free API. Researchers loved it because you could query historical data (even deleted content in some cases) and retrieve large batches more easily than Reddit’s API (which pages through results only 100 at a time).

However, in 2023 Reddit restricted Pushshift’s access. By early 2025, **Pushshift’s API for public use has been shut down** for general purposes. Reddit now allows Pushshift access only for moderators (through a special approval process) – meaning if you are a Reddit moderator and need to search your subreddit’s history, you can apply for Pushshift access. For OSINT outside of moderation, the free Pushshift API is no longer available. Pushshift’s datasets are still out there (they periodically dump data to archives), but you’d have to obtain those large data files and query them offline.

**Other Unofficial Methods:**
Reddit’s own web interface can be scraped, but since the official API is generous, that’s less common. One could use the old reddit.com JSON feeds (e.g., adding `.json` to any reddit URL often gives JSON data). For example, `https://old.reddit.com/user/<name>.json` will list the user’s recent posts in JSON without OAuth – though limited by the 10 requests/min rule. This is a quick hack if you don’t want to get an API key, but it’s inherently rate-limited and not as robust as using OAuth calls.

**Data from Reddit and Freshness:** Reddit data obtained via API or scraping is real-time (for live posts/comments). The API even has an optional **websocket** (the /api/place live threads, etc.) and the **new pushshift replacement** in development might allow live streaming. But for OSINT, typically one pulls what is currently there. If a user deleted a post or comment, the official API will not return it (Pushshift might have it if it was scraped prior to deletion, but again, not easily accessible now). Reddit’s API doesn’t provide direct user-to-user relationships (e.g., you can’t get a list of a user’s friends or followers – Reddit does have followers now, but no API endpoint for that as of now).

**Important**: A user’s **comment and post history** via the API might be incomplete if they’ve chosen to delete or overwrite their history (some users use bots to mass-delete old comments). If you suspect that, Pushshift archives could be the only source, but since the API is restricted, you may rely on cached Google results or other indirect ways to find deleted content.

Below is a table summarizing Reddit data access options:

**Table 5 – Reddit API vs Unofficial Methods**

| Method                                     | Type                       | Data Accessible                                                                                                                                                                     | Auth Requirements                                                                                                  | Limitations & Notes                                                                                                                                                                                                                | Status (2025)                                                                                                                                                 |
| ------------------------------------------ | -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Reddit API** (official) via PRAW or HTTP | Official API               | Virtually all **public Reddit content**: user profile info (karma, bio), user’s posts and comments, subreddit posts, comment threads, search results. Supports reading and posting. | **OAuth token** (free via Reddit app registration).                                                                | **Free tier:** \~100 requests/min (more than enough for most OSINT) and up to 600 requests in 10 min bursts. Heavy usage may incur fees. Reddit’s API requires abiding by terms (e.g., not redistributing personal data at scale). | **Active** – Remains one of the most open APIs. Post-2023 rules mostly impact very high-volume applications. For moderate use, it’s accessible and cost-free. |
| **Pushshift API** (unofficial archive)     | Unofficial API (3rd-party) | Historical Reddit data (all posts/comments since 2005, including those later deleted) in big data dumps. Was great for searching old data or large scale analytics.                 | **Not applicable** (was open, now closed).                                                                         | As of 2024, **public API shut down**. Now only available to certain moderators by special request. Historical data accessible via published dumps, but requires technical effort to use (no live API).                             | **Inactive for public use** – OSINT investigators can no longer query Pushshift freely. Must rely on official API or find cached data elsewhere.              |
| **Old Reddit HTML/JSON scraping**          | Unofficial (web scrape)    | Recent posts/comments from a user or subreddit via web. (Reddit’s pages can be fetched as HTML or `.json`.)                                                                         | **None** for public content. (Can use without login, though logged-in cookie might increase call limits slightly.) | Useful for quick pulls or if avoiding OAuth, but limited to 10 req/min without auth. Also, some data (like NSFW content) won’t show without a logged-in cookie with preferences.                                                   | **Active** – Reddit’s site can still be scraped. However, given the ease of API, this is secondary. It’s a fallback if one can’t use the API keys.            |
| **Web Interface (pushshift alternatives)** | Unofficial (web tools)     | Some websites (e.g., Reddit Explore, GummySearch) cache or search Reddit content for OSINT purposes.                                                                                | **None** to user (they provide a UI).                                                                              | These rely on either their own API keys or cached data. Often have their own rate limits or fees for extensive use.                                                                                                                | **Active** – various tools exist, but they often ultimately use the official API under the hood or have limited scope.                                        |

*Table 5: Reddit official API vs unofficial data access methods.*

---

## LinkedIn

LinkedIn is a professional network with valuable OSINT information (employment history, education, connections). However, LinkedIn’s official API is extremely restricted – effectively *closed* except to approved partners – so practitioners rely on scraping or third-party services.

### Official API Access (LinkedIn)

LinkedIn’s official APIs (often referred to as the **LinkedIn Marketing Developer Platform** or **Talent Solutions APIs**) are **not open to the public** in the way other platforms’ APIs are. Key points:

* **Partner-Only API:** To access LinkedIn data via API, one must be an approved LinkedIn partner. The criteria are strict: LinkedIn typically expects a partnership where the API use case doesn’t compete with LinkedIn and often involves paying large fees. It’s mentioned that being a partner might entail a **five-figure monthly spend** with LinkedIn. This effectively excludes individual researchers or small OSINT projects.

* **Limited Endpoints:** Even for partners, the available endpoints are focused on specific tasks like posting jobs, retrieving analytics for your company page, or looking up profile data for recruiting purposes (with the profile owner’s consent). The **People API** (to get profile info) exists but is very restricted and mostly intended for the person’s own profile or with strong user consent. **There is no open endpoint to arbitrarily fetch someone’s profile by username** or URL in the official API unless you go through the partner program.

* **Compliance & Privacy:** LinkedIn is very conservative with data access due to user privacy and the sensitivity of professional info. They explicitly forbid scraping or unauthorized use in their terms, and they have been known to take legal action against companies scraping at scale (though notably, a 2019 court ruling (hiQ Labs vs LinkedIn) found that scraping *public* LinkedIn profiles does not violate the CFAA – essentially making it legally safer to scrape public data). Still, LinkedIn itself does not provide an API for that.

* **Rate limits & tiers:** Since the official API isn’t broadly available, LinkedIn doesn’t publish general rate limits or free tiers. Each partner likely negotiates their usage. For small developers, there’s effectively no free tier to speak of (the old v1 API keys that some developers had were deprecated long ago).

**Bottom line:** Assume **no official API access** for normal OSINT needs on LinkedIn. Investigators must turn to unofficial methods.

### Unofficial and Scraping Methods (LinkedIn)

**Web Scraping and Reverse-Engineered APIs:** LinkedIn’s website can be navigated with automation tools, and its web requests (particularly the internal JSON endpoints) have been reverse engineered in various libraries.

* **linkedin-api (Tom Quirk’s Python library):** This is a well-known unofficial API wrapper for LinkedIn. It does not require an official API key; instead it uses a **normal LinkedIn account’s credentials** to log in and then calls the same internal APIs that your browser does when you use LinkedIn. For instance, LinkedIn’s web app uses an API called “Voyager”. The `linkedin-api` library can search for profiles, retrieve profile details (name, headline, experience, education, etc.), and even send connection requests or read messages. Essentially, it wraps most site functionality: **“Search profiles, send messages, find jobs and more in Python. No official API access required.”**. It achieves this by managing session cookies and crafting HTTP requests with the necessary headers (including CSRF tokens that LinkedIn expects).

  *Data available:* Using such a library, an OSINT investigator can:

  * Perform a search query by name or company and get a list of matching profiles (with limited info like names and titles).
  * Fetch a person’s full profile details (the data you see on their profile page: job history, education, skills, etc.), subject to your account’s visibility (if you’re not connected, some info might be hidden or abbreviated).
  * Retrieve posts made by a user or articles they wrote (if any), as well as their recent activity if visible.
  * Retrieve company pages info, job listings, etc., if relevant to the investigation.

  Authentication is done by providing a LinkedIn username and password for a bot account, or by supplying session cookies from a browser where you logged in. The library then mimics a real user’s actions via the API endpoints.

* **Selenium/Playwright automation:** Some opt to use a headless browser to scrape LinkedIn because LinkedIn has anti-bot measures like detecting rapid or unusual viewing of profiles. Automating a browser can help you navigate as a human would – for example, log in, then visit a profile URL, scroll to load all sections (Experience, Education, etc.), and parse the HTML. This is slower and heavier than using the API calls, but can sometimes bypass detection (especially if one introduces random delays, etc.). It’s also necessary for things like capturing **who viewed this profile** or other interactive elements if needed (though those are less relevant for OSINT outward data gathering).

* **Third-party scrapers and services:** There are tools like **PhantomBuster** or **Apify** which provide pre-built LinkedIn scraping “actors” (like getting all employees of a company, or all profiles from a search). These usually use headless browsers under the hood. They can save time for an investigator not wanting to code, but often have usage limits or costs. There are also datasets resold by companies like People Data Labs, which contain parsed LinkedIn profiles (but those data can be outdated, e.g., profiles “no older than 29 days” as one source indicates).

**Challenges:** LinkedIn is known to be aggressive in blocking. If an account scrapes too much, it may get logged out or have to pass CAPTCHAs or get restricted. Also, LinkedIn pages have content that’s sometimes personalized (like if you’re 3rd degree away, you might see limited info). So, an OSINT scrape might not get everything unless the account has a connection or at least group commonality. However, a lot of profile info (name, headline, current position) is public by default.

There’s also the legal dimension: LinkedIn tried to stop scrapers but the current legal precedent in the U.S. allows scraping of publicly viewable profiles. Still, scrapers should avoid logged-in scraping if trying to argue the data is public (since truly public profiles can be seen without login, but LinkedIn now often forces login to see more than a snippet). Many OSINT investigators proceed with logged-in scraping anyway due to practicality.

**Data accessible via scraping:** Practically everything on a target’s profile that your fake account can see: employment history (companies, titles, dates), education, skills, certifications, recommendations, etc. You can also gather their connections count (LinkedIn shows an approximate if >500) but not the list of connections (that’s private except to the user). However, some people’s connections are visible if they allow it – scrapers can check if the “Connections” section is accessible. Also, any **posts or activity** of that user that are public (like or comment on someone’s post) could be seen if you navigate to their “Activity” section. There’s no straightforward API for that, but one could scroll the Activity feed.

**Freshness:** It’s real-time as of the scrape moment. If a user updates their profile, a new scrape will get the new info. There’s no caching by LinkedIn unless the library caches it.

Below, Table 6 compares the (mostly theoretical) official API vs known unofficial methods:

**Table 6 – LinkedIn Data Access Options**

| Method                                                              | Type                       | Data Accessible                                                                                                                                                                  | Auth Requirements                                                            | Limitations & Notes                                                                                                                                                                                              | Status (2025)                                                                                                                                               |
| ------------------------------------------------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **LinkedIn Official API** (People/Jobs API)                         | Official API (closed)      | Profile data for authorized users (name, headline, etc), company pages, job postings. *No arbitrary profile lookup unless partner.*                                              | **LinkedIn Partner Program** – must be approved and likely pay fees.         | Effectively inaccessible to OSINT individuals. Only large enterprise partners (e.g., recruiters or sales platforms) get this. Even then, data usage is strictly regulated.                                       | **Active (Restricted)** – Exists for select partners. Not available for general use.                                                                        |
| **linkedin-api (unofficial Python SDK)**                            | Unofficial (scrape API)    | User profiles (full details visible to your account), profile search results, network data (connections if visible), messaging, posts. Basically, site functionality via Python. | **LinkedIn login** (email & password for a bot account, or session cookies). | No API key needed. Subject to LinkedIn’s anti-scraping defenses: heavy use can lead to account bans. Best to use slowly and with a realistic user agent.                                                         | **Active** – widely used tool. As of 2025, it remains functional in extracting profile data and more, though users must handle occasional login challenges. |
| **Browser Automation (Selenium)**                                   | Unofficial (web automate)  | Any profile info and search results that a logged-in account can see: work history, education, etc., and possibly screenshots or PDFs of profiles.                               | **LinkedIn account** (sock-puppet)                                           | Slow but can solve visual layout issues (like expanding sections). Need to mimic human behavior to avoid detection (random pauses, etc.). Good for one-off profile scraping in depth.                            | **Active** – Used when coding against the API is not desired. Effective on a small scale (tens of profiles), but not scalable.                              |
| **Third-Party Data Providers** (e.g. PeopleDataLabs, Proxycurl API) | Unofficial (data reseller) | Aggregated LinkedIn profile data (often hundreds of millions of profiles, with fields like name, title, company, last update).                                                   | **API Key (paid)** – one needs to buy access.                                | These services have compiled LinkedIn info (likely via large-scale scraping). Data might be slightly stale (weeks or months old). They can be expensive but provide bulk data without running scrapers yourself. | **Active** – Available for those who can pay. Often used in background check industry. Not “free” OSINT, but worth noting as an option.                     |

*Table 6: LinkedIn official vs unofficial data access for OSINT.*

---

## YouTube

YouTube, being a Google platform, has a well-documented official API that is relatively open for non-commercial use (with quotas). OSINT often involves gathering data on YouTube channels (user bios, uploaded videos, comments on those videos, etc.). Both the official API and scraping methods can be used.

### Official API Access (YouTube Data API)

The **YouTube Data API v3** is an official API that provides access to YouTube content and metadata. It is one of the more accessible APIs in terms of availability and generous free usage limits:

* **API Key and OAuth:** You can access read-only data with just an API key (no user auth needed) if it’s public data. For example, listing videos on a public channel or fetching comments on a public video only needs your project’s API key. For actions like posting comments or accessing private videos, OAuth (user login) is required. For OSINT, we typically deal with public data and an API key suffices.

* **Data Available:** The Data API covers:

  * **Channel information:** Using `channels.list` you can get channel details by ID or by username (if old legacy username). This returns title, description (bio/about text), subscriber count (if not hidden by the channel), total view count, creation date, etc. You can also get the uploads playlist ID.
  * **Videos and Playlists:** You can list all videos on a channel via the uploads playlist or by searching. For each video, you get title, description, upload date, view count, like count, comment count, etc.
  * **Comments:** There’s a `commentThreads.list` endpoint to fetch comments on a video (and replies to those comments). This is crucial for OSINT if analyzing what discussions occur on a particular video.
  * **Search:** You can search videos or channels by keywords. For instance, you could find a channel by a person’s name (though search results need filtering/logic to pick the right one).
  * **Subscriptions (limited):** If you authenticate as a user, you could see that user’s subscriptions or subscribers, but for OSINT on someone else, that’s not applicable unless you have their account (which we don’t).

  Essentially, anything you can see publicly on YouTube, you can probably get via the API.

* **Quotas and Rate Limits:** Google uses a quota system with “units” per day. By default, each API project gets **10,000 units per day** for free. Each API call has a cost (e.g., a video list might be 1 unit per video returned, a search query is 100 units). 10,000 units/day is quite high – for example, you could fetch details for 10,000 videos in a day on the free quota. Most OSINT tasks (maybe grabbing a couple hundred videos and their comments) will easily fit in this quota. If more is needed, you can request an increase, but Google may require you to justify usage and comply with their policies (especially if you’re showing data publicly).

  There is **no direct cost** for using the API up to the quota. If you needed to massively exceed it, you’d have to go through an audit and maybe a paid arrangement or just abide by the cap.

* **Live Data & Limitations:** The API returns current data. One thing to note: if a channel hides their subscriber count, the API will not return it (it’ll indicate it’s hidden). Also, the API does not return the list of subscribers to a channel for privacy reasons. It also doesn’t provide an easy way to get all videos a user has commented on (you’d have to use the comments endpoints on each video or a search on comment threads which isn’t available). But a channel’s own content and its comment sections are fully accessible if public.

The YouTube Data API is very useful for OSINT – e.g., you can programmatically get all videos by a person, read all the comments to see if there are any interesting patterns or links, get the person’s channel description (which often contains other social media links), etc., all within the free quota.

### Unofficial and Scraping Methods (YouTube)

While the official API suffices for most needs, there are also unofficial methods:

* **Web Scraping (HTML/JSON):** YouTube’s web front-end loads data via internal APIs (like the “innertube” AJAX calls). Some data can be accessed by simply adding parameters to URLs (for example, `https://www.youtube.com/feeds/videos.xml?channel_id=<ID>` gives an RSS feed of videos, albeit with limited info). More commonly, scrapers either parse the HTML of YouTube pages or use alternative frontends:

  * **Invidious:** This is an open-source front-end for YouTube that many OSINT people use. Instances of Invidious provide their own JSON API for YouTube. For instance, you can get channel info by `https://yewtu.be/api/v1/channels/<channelId>` or comments by `.../api/v1/comments/<videoId>`. This can be easier than using the official API and avoids the quota limit, but it depends on third-party instances which might themselves get blocked or rate-limit usage. (However, note that Invidious itself scrapes or uses the official API; recently, Google has taken measures against Invidious.)
  * **yt-dlp / youtube-dl:** These are tools primarily for downloading videos, but they also retrieve metadata. `yt-dlp`, in particular, can fetch video details and has an option to download comments. Using yt-dlp programmatically (it has a Python library interface) allows you to extract video metadata (title, description, upload date, view count) and even the list of comments without needing an API key. It effectively scrapes the web player’s comment section, solving the needed calls. OSINT investigators sometimes use yt-dlp to quickly grab all comments from a video for text analysis.
  * **YouTube Web in headless browser:** One could also automate a browser to go to a video page, click on the “Show more” to expand description, scroll through comments and collect them. But since the official API and yt-dlp cover these, headless browser is rarely needed except maybe to render certain dynamic content or transcripts.

* **Transcripts/Subtitles:** One piece of data not directly in the Data API is the auto-generated transcript of a video. However, these can be accessed via web scraping or tools. For instance, youtube-dl/yt-dlp can download subtitles (including auto-captions if available) for videos. This is relevant OSINT if you want to search what was said in videos.

* **Limitations of Scraping:** If using non-API methods, you have to be mindful of Google’s anti-bot (though YouTube is generally reachable without login for public data, and Google doesn’t block as aggressively for reads). The main drawback is efficiency – for example, the official API can give you 100 comments in one query, whereas scraping might require scrolling and retrieving page by page. On the flip side, certain data like exact comment reply threads might be easier to get via scraping (the official API does allow comment threads and then replies, but it might get complicated with many nested replies).

**Data & Freshness:** Any method will yield up-to-date info (e.g., current view count). The official API might have a few minutes delay for metrics like view count or comment count due to caching, but it’s essentially real-time. Scraping the site shows what a user sees, which is as real-time as it gets.

**Table 7 – YouTube Data Access Options:**

**Table 7 – YouTube Official API vs Unofficial Tools**

| Method                                    | Type                 | Data Accessible                                                                                                                                                                                                          | Auth Requirements                                                           | Limitations & Notes                                                                                                                                                                                                                                         | Status (2025)                                                                                                                                                                                        |
| ----------------------------------------- | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **YouTube Data API v3**                   | Official API         | Channel info (name, description, creation date, subscriber count); Videos (titles, descriptions, stats, etc); Comments on videos; Search results by keyword.                                                             | **API Key** for public read (free). OAuth if needing user-specific actions. | **Free quota 10k units/day** (e.g., \~10k video info fetches). Very comprehensive – covers most needs. Some data like subscriber lists or video transcripts not available due to privacy.                                                                   | **Active** – Robust and generous. Preferred for structured data retrieval. Widely used in OSINT and analytics.                                                                                       |
| **yt-dlp / youtube-dl (scraping)**        | Unofficial (scrape)  | Video metadata (all info visible on YouTube page: title, description, views, likes, etc) and **all comments** on a video (using comments extraction). Can also get channel’s videos list and download content if needed. | **None** (no login, uses YouTube’s public web).                             | No API key needed, bypasses Google quotas. However, heavy comment scraping on many videos might trigger temporary blocks (rare). Also, cannot directly search – you need a video or channel ID as input (though channel URL can be used to get all videos). | **Active** – Maintained. As of 2025, still able to retrieve comments and video data reliably. Good for one-off data dumps (e.g., get all comments from a specific controversial video for analysis). |
| **Invidious API** (third-party front-end) | Unofficial (alt API) | Similar to official: channel info, video lists, comments, etc, via alternative API endpoints.                                                                                                                            | **None** (use public Invidious instance).                                   | Dependent on third-party servers which may be slow or go down. Data might sometimes lag if instance caches results. Google has taken steps to limit Invidious; some instances get blocked.                                                                  | **Partially Active** – Some Invidious instances still up, but reliability is not guaranteed long-term due to legal/technical pressures.                                                              |
| **Direct Web Scraping (HTML/Browser)**    | Unofficial (web)     | Any information on the web interface: e.g., the *About* page of a channel (which shows bio and stats), video page HTML (which has description and initial comments), etc.                                                | **None** for public data. (No login required for YouTube public content.)   | More cumbersome than using API or specialized tools. For example, to get all comments, you’d have to scroll and click “Load more” repeatedly. Only use if avoiding API and other tools for some reason.                                                     | **Active** – Always possible as a fallback, but typically one uses the above methods for efficiency.                                                                                                 |

*Table 7: YouTube data via official API vs unofficial scraping.*

---

## Telegram

Telegram is a messaging platform often used in OSINT investigations (for example, extremist group channels, or public chat groups). Telegram provides **official APIs** for both bots and users (clients), which are fairly open, and many open-source tools exist to leverage them.

### Official API Access (Telegram)

Telegram has two types of API:

* **Bot API (HTTP)** – Allows creating a bot account that can send/receive messages in chats.
* **Client API (MTProto)** – The full Telegram API that a regular user client (like the Telegram app) uses. Libraries like Telethon or Pyrogram implement this.

For OSINT data collection, the **Client API** is more relevant because it can retrieve data from any public group or channel that a user could join. The Bot API is more limited (bots cannot read arbitrary chats unless added to them, and even then they only see messages sent after they join).

**Telegram Client API (MTProto):** This API is officially provided by Telegram for developers to build their own Telegram clients. It requires a phone number to login (just like the official app). Key points for OSINT:

* Once logged in (as a user account), you can access any public channel or group’s content. For example, you can use Telethon library to **join a public channel** and then **scrape all its messages**. You can also join public groups and scrape messages, or even enumerate members (if the group’s member list is visible).
* You can look up user profiles by username or ID. For instance, `get_entity(@username)` in Telethon will return a user’s basic info (Telegram ID, name, username, and perhaps profile photo and “about/bio” text). Note that if the user’s privacy settings are strict (e.g., nobody can see their last seen, etc.), you still get their name and username, but not much else. However, if they have a public username, their profile *about* (bio) is usually visible.
* The API can fetch a user’s **profile photo** (the latest one, or even all past ones, though that might be more than needed).
* If investigating a specific user’s activity, one approach is to **search for that user’s messages in groups**. The API has methods to search messages in a chat by a given sender. You’d have to be in those groups though. If the user participates in a known public group, you can gather all their messages from that group with one query.

**Rate Limits:** Telegram’s client API is fairly generous. There isn’t a documented fixed rate limit like “X calls per second,” but if you hit the servers too fast, you may get FloodWait errors (which Telethon handles by pausing as needed). Typical usage like iterating over 10,000 messages of a channel is fine – it will just take some time with rate delays. The API is free; you just need to get an API ID and hash (which any user can obtain from my.telegram.org).

**Bot API:** A Telegram bot can do some limited OSINT tasks:

* If added to a group, it can read the messages in that group (so you could use a bot to monitor new messages in a group of interest).
* It can’t access user profiles unless those users interact with it. So a bot can’t arbitrarily pull info on a username.
* Bots also can’t join channels on their own (they need to be invited to groups; for channels, bots can’t read channel posts unless the channel has a feature to allow bot as admin, which is uncommon).
* Bot API has strict rate limits (like \~100 messages per second per bot, and retrieval of updates 1 per second, etc.), but these are generally okay for moderate monitoring.

For comprehensive data collection, OSINT uses the client API with a user account because it can join any number of channels, etc.

### Unofficial Methods (Telegram)

Because the official client API is available and unrestricted, there’s not much need for “unofficial” scraping of Telegram (like HTML scraping). Telegram does have a web version and a t.me preview for channels, but those usually only show recent messages and often require clicking “next”. It’s far easier to use the API.

**Relevant Tools/Libraries:**

* **Telethon (Python):** An asyncio-based library for the Telegram client API. Very popular in the OSINT community. With Telethon you can easily do things like:

  * Connect with your API ID and phone (it will send the code, you enter it once).
  * Use `client.get_messages(channel, limit=n)` to fetch messages.
  * Use `client.iter_participants(group)` to get members of a group (if allowed).
  * Use `client.get_entity('username')` to resolve an @username to a user or channel object.
  * It basically wraps all MTProto functions (sending messages, editing profiles, etc. – though for OSINT you mostly use the read functions).
    Telethon handles rate limits by automatically waiting if a flood wait occurs.
* **Pyrogram (Python):** Another client API library (more user-friendly, uses async optionally or can be sync). Either Telethon or Pyrogram can be used similarly.
* **Telegram CLI (telepathy):** A command-line tool to interface with Telegram (less used nowadays due to easier Python libs).

**Data Accessible via client API:**

* Public **Channels**: You can get the entire message history of a channel (all posts from the beginning). This is useful for archiving propaganda channels, for instance.
* Public or private **Groups**: If the group is public, you can join and scrape history. If private but you have an invite link or some access, you could also join (with the human step of accepting the invite) and then let the API collect data.
* **User info**: For any user that your account encounters (e.g., in a group or by username search), you can get their id, username, name, and bio text. You cannot get their contact list unless you are actually logged into their account (which you aren’t), so you don’t get “social graph” beyond what’s visible in groups.
* **Files**: The API can also download media (photos, videos, documents) from chats. This can be relevant if investigating files shared.
* **Real-time**: You can set up event handlers to listen for new messages in certain chats and collect them as they come (good for monitoring going forward).

**Current Status:** Telegram’s APIs are stable and in use. There was a significant addition in recent years (like adding support for Stories in 2023 to the API), but nothing that hinders scraping messages. Telegram encourages bots and custom clients, so unlike other platforms, you’re not breaking rules by using these APIs (as long as you’re not spamming or violating privacy laws).

One must keep the API credentials secure and not abuse them. If you do crazy scraping (join thousands of groups rapidly), Telegram might ban that account for spam suspicion. But normal OSINT collection is typically fine.

**Table 8 – Telegram Data Access Options:**

**Table 8 – Telegram API (Client/Bot) vs Other Methods**

| Method                                          | Type                | Data Accessible                                                                                                                                                                                   | Auth Requirements                                                                                   | Limitations & Notes                                                                                                                                                                                                                                                          | Status (2025)                                                                                                                                                                |
| ----------------------------------------------- | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Telegram Client API** (via Telethon/Pyrogram) | Official API (user) | Virtually all content a normal user could see: messages in public channels and groups, user profile info (name, username, bio), member lists of groups, etc. Also allows downloading media files. | **Telegram account login** (phone number + code). Need Telegram API ID & hash (free from Telegram). | Can only access chats that the account joins or has access to. Some large channels don’t allow seeing old messages unless joined (but you can join most). Must respect Telegram’s flood limits – e.g., avoid joining too many groups too fast or scraping extremely quickly. | **Active** – Extremely useful and permitted by Telegram. As of 2025, it’s a primary way to gather Telegram OSINT. Libraries like Telethon fully support latest API features. |
| **Telegram Bot API**                            | Official API (bot)  | Messages and updates in chats where the bot is added. E.g., can monitor messages in a group chat after being invited. Can fetch basic public info of chat members when they interact.             | **Bot token** (from BotFather) – no phone number needed, but bot must be invited to targets.        | **Read limitation:** Bot only sees messages in groups from the point it’s added (cannot fetch history). Cannot join channels on its own. Privacy mode (default) might restrict reading all messages (bots can be set to see all though).                                     | **Active** – Great for setting up monitors/alerts in specific groups. Not useful for retrospective scraping since history isn’t accessible.                                  |
| **Direct scraping of t.me or Web**              | Unofficial (web)    | Limited: t.me links for public channels show recent 10 messages via web, and you can paginate by clicking. One could scrape those HTML pages.                                                     | **None** (public channels can be viewed on web without login; private require invite links).        | Not efficient for large-scale: would need to crawl page by page for history. Also doesn’t work for groups (t.me links only exist for channels and individual messages).                                                                                                      | **Active** – but rarely used given the powerful official API. Mostly an option if one refuses to use any official API keys.                                                  |

*Table 8: Telegram official APIs vs alternative scraping (the official client API is the dominant method).*

---

## Mastodon

Mastodon is a decentralized social network (part of the Fediverse) that mimics Twitter-like microblogging. It’s open-source and each server (instance) has its own API. Fortunately, Mastodon’s API is **open and standard** across instances, and many instances allow public data access without heavy restrictions. This makes OSINT on Mastodon relatively straightforward.

### Official API Access (Mastodon)

Every Mastodon instance exposes a RESTful API that clients use. The API is well-documented and **anyone can register an application** on a Mastodon instance to use it. Key features:

* **Access to Public Data:** By default, many endpoints do not even require an access token for read-only access to public information (this can depend on instance settings, but generally fetching public posts or profiles can be done without auth). However, some instances might enforce using an API token – which is easy to get by making a free account and generating an app token.

* **Data Available:**

  * **User (Account) info:** You can get account profiles by ID or by handle (once you know their instance). This includes display name, username, bio (called “note”), follower count, following count, account creation date, etc. All of this is public for any user on that instance.
  * **Statuses (Posts):** You can retrieve a user’s posts via `/api/v1/accounts/:id/statuses`. It will return recent toots (posts) with content, timestamps, attachments, etc. You can page through to get older ones. If the account is locked (requires follow approval) and your account doesn’t have access, then you won’t get their posts – same as web.
  * **Timelines:** There are endpoints for public timeline, tag timelines, etc. But for OSINT on one user, the account statuses are key.
  * **Followers/Following:** Endpoints `/accounts/:id/followers` and `.../following` list the users following or followed by the target, if not hidden. Mastodon allows users to hide their follower lists from public view; the API will respect that.
  * **Search:** There is a `/api/v2/search` that can search for accounts or hashtags. It requires an access token now for full use (due to spam prevention).
  * Because Mastodon is decentralized, if OSINT target “@user\@someinstance” is on a different server than you have an account on, you might either query that instance’s API directly or use the federated search if your instance knows about them. Often, it’s simplest to query the target’s own instance API for their data (since it definitely has it). This might require using a library that supports pointing to a specific base URL (the Mastodon.py library does).

* **Rate Limits:** Mastodon’s default rate limit is **300 requests per 5 minutes per account**. This is quite generous (that's 60 requests/minute) and is usually not hit during typical profile scraping or even iterating posts, especially since you can get 20 posts per page by default. Some instances might adjust their limits (smaller instances could lower it to reduce load, but many stick with defaults). Response headers include remaining limit info. If you hit the limit, you’d wait until the 5-min window resets. For a single account’s data, you rarely will hit this unless you pull thousands of entries quickly.

* **Authentication:** If needed, obtaining a token is easy: you can use OAuth to get a user token or, for script usage, Mastodon also supports **application tokens** (as of newer versions) or using your own account’s user token. But again, public data often comes through without it. However, search API and some actions do need auth.

Because of this openness, OSINT investigators can straightforwardly use the official API to gather data on Mastodon users. Many have done so to monitor migration from Twitter to Mastodon, for example.

### Unofficial / Scraping Methods (Mastodon)

There’s not much reason to avoid the official API given its ease. One could scrape the web HTML (Mastodon has public web profiles, e.g., `https://mastodon.social/@username` which you can parse to get latest toots), but the API gives structured JSON easily. So basically, **unofficial method = just use the official API** because it’s already open-source and intended for use.

If one did not want to use API keys at all, they could fetch `https://mastodon.social/@username.rss` – Mastodon offers RSS feeds for profiles which include recent posts. That’s another quick OSINT tip: many Fediverse platforms have RSS outputs.

**Tools/Libraries:**

* **Mastodon.py (Python):** An official Python wrapper for Mastodon’s API. You can instantiate it with an `api_base_url` (the instance domain) and either provide an access token or not if just doing public calls. It simplifies calls like `mastodon.account_search()`, `mastodon.account_statuses(account_id)`, etc. It handles pagination transparently.
* You can also just use `requests` to call the APIs directly, as they are REST+JSON and straightforward.

**Data accessible:** All *public* posts and profiles. If a user’s posts are unlisted or public, you get them. If they have followers-only posts, you wouldn’t unless your account follows them and is approved. But those cases are similar to private Twitter accounts – if you’re not authorized, you can’t scrape that content. For OSINT, typically one focuses on public figures or content that is public.

**Current Status:** Mastodon’s API is stable and continually maintained with the platform. Instances update as new features come. As of 2025, many OSINT analysts use Mastodon API to track trends or users across instances.

**Table 9 – Mastodon Data Access:**

**Table 9 – Mastodon Official API vs Scraping**

| Method                      | Type             | Data Accessible                                                                                                                                                                           | Auth Requirements                                                                                                                 | Limitations & Notes                                                                                                                                                        | Status (2025)                                                                                      |
| --------------------------- | ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| **Mastodon REST API**       | Official API     | User account info (profile details, stats); User’s posts (toots); Followers/following lists; Hashtag timelines; Instance public timeline. Basically everything that’s public on Mastodon. | **Access token optional**. Many endpoints can be called without auth (for public data). Auth needed for actions or some searches. | Per-instance rate limit \~300 requests/5min by default. Need to know which instance the user is on (federation isn’t centralized). Otherwise very few restrictions.        | **Active** – Fully open and used by many apps. Perfect for OSINT on Fediverse content.             |
| **Web scraping (HTML/RSS)** | Unofficial (web) | Public profile pages (recent posts, profile bio), public timelines. RSS feeds provide recent posts of a user.                                                                             | **None** (data is public).                                                                                                        | Might miss some structured data (e.g., precise timestamps or all pagination beyond first page if not handled). The official API or RSS is easier for structured retrieval. | **Active** – Available, but given the quality of official API, usually unnecessary to scrape HTML. |

*Table 9: Mastodon is highly accessible via its official API, reducing need for unofficial methods.*

---

## Conclusion

Across these platforms, the ease and legality of obtaining live user data vary widely. **Twitter/X** and **LinkedIn** present the toughest challenges due to strict API limitations (or outright closure) – necessitating reliance on scraping tools that must continually adapt to platform changes. **Facebook** and **Instagram** also restrict official APIs heavily, but a combination of targeted scraping libraries (like facebook-scraper, Instaloader) and creative use of available web endpoints can yield substantial OSINT data (while respecting privacy and terms of service). **Reddit** remains relatively open with an official API that covers most needs, complemented by historical data archives (now less accessible than before). **YouTube** stands out with a very robust official API for gathering channel and comment data, making it a go-to choice for structured data collection, with scraping tools as a secondary option. **Telegram** provides powerful official APIs that allow deep dives into public chat data in real-time. Finally, **Mastodon** – being an open-source federated platform – offers an easy and rich API that aligns well with OSINT, reflecting the platform’s transparent ethos.

When performing social media OSINT in 2025, investigators should choose the method appropriate for each platform’s ecosystem: use official APIs where they are available and free (e.g., Reddit, YouTube, Mastodon, Telegram) and turn to well-maintained scraping libraries for platforms that lock down data (Twitter/X, Instagram, Facebook). Always keep an eye on current status and community updates, as the tools and endpoints can change with little notice. By leveraging the strategies outlined above, an OSINT practitioner can gather up-to-date profiles, posts, and interactions across the social media spectrum, while navigating the technical and ethical constraints unique to each platform.
